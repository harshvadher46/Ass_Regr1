{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839a8fc-c6b5-4b8a-b003-17cb1c6c2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-1\n",
    "\n",
    "Simple linear regression involves predicting a target variable based on a single predictor variable.\n",
    "It assumes a linear relationship between the predictor and the target.\n",
    "\n",
    "Y = β0 + β1X + ϵ\n",
    "\n",
    "Y is the target variable.\n",
    "X is the predictor variable.\n",
    "β0 is the intercept.\n",
    "β1is the coefficient of the predictor variable.\n",
    "ϵ represents the error term.\n",
    "\n",
    "Example of simple linear regression:\n",
    "Suppose we want to predict the salary of an employee based on the number of years of experience they have. \n",
    "Here, the number of years of experience is the predictor variable, and salary is the target variable.\n",
    "\n",
    "\n",
    "Multiple linear regression, on the other hand, involves predicting a target variable based on multiple predictor variables.\n",
    "It extends the concept of simple linear regression to accommodate multiple predictors.\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 +...+ βnXn +ϵ\n",
    "\n",
    "Y is the target variable.\n",
    "X1,X2 ,...,Xn are the predictor variables.\n",
    "β0 is the intercept.\n",
    "β1,β2,...,βn are the coefficients of the predictor variables.\n",
    "ϵ represents the error term.\n",
    "\n",
    "Example of multiple linear regression:\n",
    "Suppose we want to predict the house price based on several features such as square footage, number of bedrooms, number of bathrooms, and location.\n",
    "Here, square footage, number of bedrooms, number of bathrooms, and location are the predictor variables, and house price is the target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104a1e2-44e9-4c39-acc4-d1876a35339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-2\n",
    "Linear regression relies on several assumptions for its validity.\n",
    "Violations of these assumptions can lead to inaccurate and unreliable results.\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear.\n",
    "           This means that the change in the dependent variable is proportional to the change in the independent variables.\n",
    "\n",
    "Independence of errors: The errors (residuals) should be independent of each other. In other words, \n",
    "                        there should be no correlation between consecutive errors.\n",
    "Normality of residuals: The residuals should be normally distributed. \n",
    "                        This means that the distribution of residuals should follow a bell-shaped curve when plotted.\n",
    "\n",
    "--> To check whether these assumptions hold in a given dataset, several diagnostic techniques can used:\n",
    "\n",
    "Residual plots: Plot the residuals against the predicted values. This helps in assessing linearity and homoscedasticity. \n",
    "                The plot should show a random scatter around zero with no discernible pattern.\n",
    "\n",
    "Normality tests: Conduct statistical tests such as the Shapiro-Wilk test or visually inspect the histogram or Q-Q plot of residuals to assess normality.\n",
    "\n",
    "Independence of errors: Plot the residuals against time or any other relevant variable to check for patterns or autocorrelation.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45ecdb-cff2-4743-bf67-228b66805b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-3\n",
    "In a linear regression model of the form Y = β0 + β1X + ϵ the slope (β1) represents the change in the dependent variable (Y) for a one-unit change in the independent variable ( X), holding all other variables constant. It indicates the direction and magnitude of the relationship between the independent and dependent variables. The intercept ( β0) represents the value of the dependent variable ( Y) when all independent variables are zero.\n",
    "It is the value of Y when the predictor variable X is zero.\n",
    "\n",
    "For example, let's consider a real-world scenario:\n",
    "\n",
    "Scenario: Predicting House Prices\n",
    "\n",
    "Suppose we have a dataset containing information about houses, including their sizes (in square feet) and prices (in dollars). We want to build a linear regression model to predict the price of a house based on its size.\n",
    "\n",
    "Our linear regression model can be represented as\n",
    "Y = β0 + β1 * size + ϵ\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Intercept (β0): Let's say the intercept (β0) is $50,000. This means that the predicted price of a house with zero square feet (i.e., when the size is zero) is $50,000. However, this interpretation might not be practically meaningful in this context, as houses with zero square feet do not exist.\n",
    "Slope (β1): Suppose the slope (β1) is 100. This means that for every additional square foot in the size of the house, we expect the price to increase by $100, assuming all other factors remain constant.\n",
    "So, in this example, the intercept represents the baseline price of a house, and the slope represents the rate of change in price per unit change in size.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc5943-8295-484b-9af9-88e88f1bd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-4\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function of a model by iteratively updating the model parameters in the direction of the steepest descent of the gradient.\n",
    "It is widely used in machine learning for training various types of models, including linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: Start with an initial guess for the model parameters (weights). These can be chosen randomly or using some heuristic.\n",
    "\n",
    "Calculate the Gradient: Compute the gradient of the loss function with respect to each parameter. \n",
    "                        The gradient indicates the direction of the steepest increase of the function at the current point. In other words, it tells us how the loss function changes as each parameter changes.\n",
    "\n",
    "Update Parameters: Adjust the parameters in the direction opposite to the gradient to minimize the loss function.\n",
    "                   This adjustment is made by subtracting a fraction of the gradient from the current parameter values, scaled by a parameter called the learning rate. \n",
    "                   The learning rate determines the step size of each update.\n",
    "\n",
    "Iterate: Repeat steps 2 and 3 until convergence criteria are met, such as reaching a maximum number of iterations or when the change in the loss function becomes sufficiently small.\n",
    "\n",
    "Gradient descent is used in machine learning to train models by adjusting their parameters to minimize the difference between predicted and actual values (i.e., minimize the loss function).\n",
    "By iteratively updating the parameters based on the gradient of the loss function, gradient descent helps models converge to optimal parameter values, leading to better performance on unseen data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba5040-422a-4b11-9a68-cf0488e1c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-5\n",
    "Multiple linear regression is an extension of simple linear regression that allows for the prediction of a target variable based on two or more predictor variables. \n",
    "In multiple linear regression, the relationship between the target variable (often denoted as Y) and the predictor variable often denoted as (x1,x2,x3,...)\n",
    "\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Predictor Variables: In simple linear regression, there is only one predictor variable, while in multiple linear regression, there are two or more predictor variables.\n",
    "Model Complexity: Multiple linear regression models are more complex than simple linear regression models because they consider the combined effect of multiple predictors on the target variable.\n",
    "Interpretation of Coefficients: In simple linear regression, there is only one coefficient (slope) to interpret, representing the change in the target variable for a one-unit change in the predictor variable. In multiple linear regression, there are multiple coefficients, each representing the change in the target variable for a one-unit change in the corresponding predictor variable, holding all other predictors constant.\n",
    "Model Evaluation: Model evaluation techniques such as adjusted R square and analysis of variance (ANOVA) are used to assess the overall fit of the multiple linear regression model, considering the contributions of all predictor variables. In contrast, in simple linear regression, these techniques are simpler as there is only one predictor variable to consider.\n",
    "\n",
    "Overall, multiple linear regression allows for the modeling of more complex relationships between multiple predictor variables and a target variable,\n",
    "making it a powerful tool in statistical analysis and predictive modeling.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0e0fc-ae1f-4bef-a91b-95fb1b8515fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-6 \n",
    "Multicollinearity refers to the presence of high correlation between two or more predictor variables in a multiple linear regression model.\n",
    "It can cause issues in the estimation of the regression coefficients and lead to unreliable results. Multicollinearity does not affect the predictive accuracy of the model,\n",
    "but it affects the interpretability of the individual coefficients.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of predictor variables. High correlation coefficients (typically greater than 0.7 or 0.8) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Compute the VIF for each predictor variable. VIF measures how much the variance of a coefficient is inflated due to multicollinearity. A VIF greater than 10 or 5 is often considered indicative of multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove Redundant Variables: If two or more variables are highly correlated, consider removing one of them from the model. Choose the variable that is less theoretically meaningful or less important for the research question.\n",
    "\n",
    "Feature Selection: Use feature selection techniques such as backward elimination, forward selection, or stepwise regression to select a subset of predictor variables that are most relevant for predicting the target variable.\n",
    "\n",
    "Principal Component Analysis (PCA): Transform the predictor variables into a set of uncorrelated principal components using PCA. This can help mitigate multicollinearity while retaining most of the variation in the data.\n",
    "\n",
    "Regularization: Apply regularization techniques such as Lasso (L1 regularization) or Ridge (L2 regularization) regression, which penalize large coefficients and can help stabilize them in the presence of multicollinearity.\n",
    "\n",
    "Data Collection: Collect more data to reduce the effect of multicollinearity. With a larger sample size, the estimation of coefficients becomes more stable.\n",
    "\n",
    "Interaction Terms: If multicollinearity arises due to interactions between variables, consider adding interaction terms to the model to explicitly capture these relationships.\n",
    "\n",
    "--> By detecting and addressing multicollinearity, you can improve the stability and interpretability of the multiple linear regression model, leading to more reliable and accurate results.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656357c-b707-4f54-8575-5fd482b97553",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q-7\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable (predictor) and the dependent variable (response) is modeled as an nth degree polynomial. \n",
    "It is an extension of linear regression, allowing for more flexible relationships between the variables.\n",
    "\n",
    "Mathematically, a polynomial regression model of degree n can be represented as:\n",
    "Y = β0 + β1X + β2X2 + β3X3 +...+ βnXn +ϵ\n",
    "\n",
    "Where:\n",
    "Y is the dependent variable.\n",
    "X is the independent variable.\n",
    "β0,β1,β2,...,βn are the coefficients of the polynomial terms.\n",
    "ϵ represents the error term.\n",
    "\n",
    "Polynomial regression models can capture nonlinear relationships between \n",
    "the predictor and response variables, allowing for curves and bends in the fitted line.\n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "Model Complexity: Polynomial regression allows for more complex relationships between the predictor and response variables compared to linear regression. Linear regression assumes a linear relationship, while polynomial regression can capture curves and bends.\n",
    "\n",
    "Number of Parameters: In linear regression, there are only two parameters to estimate: the intercept and slope. In polynomial regression, the number of parameters increases with the degree of the polynomial. \n",
    "                      A quadratic polynomial has three parameters (intercept, linear coefficient, quadratic coefficient), a cubic polynomial has four parameters, and so on.\n",
    "\n",
    "Overfitting: Polynomial regression models can be prone to overfitting, especially when the degree of the polynomial is high. Overfitting occurs when the model fits the noise in the data rather than the underlying relationship.\n",
    "             Regularization techniques such as ridge regression or lasso regression can be applied to mitigate overfitting.\n",
    "\n",
    "Interpretability: In linear regression, the interpretation of coefficients is straightforward: each coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "                  In polynomial regression, interpretation becomes more complex as the coefficients represent the effect of the polynomial terms, making it harder to interpret the relationship between the variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c213e93-707e-4c6c-be19-7eeeb01daac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "--> Advantages of Polynomial Regression over Linear Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture nonlinear relationships between the predictor and response variables, allowing for curves and bends in the fitted line. This flexibility makes it more suitable for modeling complex relationships that cannot be adequately captured by linear regression.\n",
    "\n",
    "Improved Fit: In cases where the relationship between the variables is nonlinear, polynomial regression can provide a better fit to the data compared to linear regression. It can reduce bias and improve the model's predictive accuracy by capturing more variation in the data.\n",
    "\n",
    "Higher Order Relationships: Polynomial regression allows for modeling higher order relationships between the variables. By including polynomial terms of higher degrees, it can capture more intricate patterns in the data.\n",
    "\n",
    "--> Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "Overfitting: Polynomial regression models with high degrees of polynomial terms are prone to overfitting, especially when the sample size is small. Overfitting occurs when the model fits the noise in the data rather than the underlying relationship, leading to poor generalization to unseen data.\n",
    "\n",
    "Increased Complexity: As the degree of the polynomial increases, the model becomes more complex with a larger number of parameters to estimate. This complexity makes interpretation more challenging and increases the risk of model instability.\n",
    "\n",
    "Extrapolation Issues: Polynomial regression models may not generalize well beyond the range of the observed data. Extrapolating predictions outside the range of the data can lead to unreliable results, especially in regions where the model has not been validated.\n",
    "\n",
    "When to Prefer Polynomial Regression:\n",
    "\n",
    "Nonlinear Relationships: When the relationship between the predictor and response variables is nonlinear, polynomial regression is preferred over linear regression. It can capture curves, bends, and other nonlinear patterns in the data.\n",
    "\n",
    "Higher Order Relationships: When there is evidence of higher order relationships between the variables, such as quadratic or cubic effects, polynomial regression can provide a more accurate representation of the data.\n",
    "\n",
    "Limited Sample Size: In cases where the sample size is relatively large, polynomial regression can be used cautiously to capture complex relationships without overfitting. Regularization techniques such as ridge regression or lasso regression can help mitigate overfitting.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b4f98-95ff-4026-a02a-b460d18a7b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
